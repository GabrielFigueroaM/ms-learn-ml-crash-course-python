{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "colab": {
      "name": "11. Recurrent Neural Networks - Python.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "jPictl7DttTu"
      },
      "source": [
        "Exercise 11 - Recurrent Neural Networks\n",
        "========\n",
        "\n",
        "A recurrent neural network (RNN) is a class of neural network that excels when your data can be treated as a sequence - such as text, music, speech recognition, connected handwriting, or data over a time period. \n",
        "\n",
        "RNN's can analyse or predict a word based on the previous words in a sentence - they allow a connection between previous information and current information.\n",
        "\n",
        "This exercise looks at implementing a LSTM RNN to generate new characters after learning from a large sample of text. LSTMs are a special type of RNN which dramatically improves the model’s ability to connect previous data to current data where there is a long gap.\n",
        "\n",
        "We will train an RNN model using a novel written by H. G. Wells - The Time Machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aumhd4WLtyUN",
        "outputId": "b186e478-53c2-43d6-fc14-0c9b1668a3ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9lSQnm4ttTy"
      },
      "source": [
        "Step 1\n",
        "------\n",
        "\n",
        "Let's start by loading our libraries and text file. This might take a few minutes.\n",
        "\n",
        "#### Run the cell below to import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "-6dZZptZttT1"
      },
      "source": [
        "%%capture\n",
        "# Run this!\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, LSTM\n",
        "from keras.callbacks import LambdaCallback, ModelCheckpoint\n",
        "import numpy as np\n",
        "import random, sys, io, string"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65b5OAUvttUF"
      },
      "source": [
        "#### Replace the `<addFileName>` with `The Time Machine`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "eDLqv_4PttUH",
        "outputId": "e0a27e84-6452-4d61-861a-853dff0b34c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "###\n",
        "# REPLACE THE <addFileName> BELOW WITH The Time Machine\n",
        "###\n",
        "text = io.open('/content/drive/My Drive/data/Data/The Time Machine.txt', encoding = 'UTF-8').read()\n",
        "###\n",
        "\n",
        "# Let's have a look at some of the text\n",
        "print(text[0:198])\n",
        "\n",
        "# This cuts out punctuation and make all the characters lower case\n",
        "text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "# Character index dictionary\n",
        "charset = sorted(list(set(text)))\n",
        "index_from_char = dict((c, i) for i, c in enumerate(charset))\n",
        "char_from_index = dict((i, c) for i, c in enumerate(charset))\n",
        "\n",
        "print('text length: %s characters' %len(text))\n",
        "print('unique characters: %s' %len(charset))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿The Time Traveller (for so it will be convenient to speak of him) was expounding a recondite matter to us. His pale grey eyes shone and twinkled, and his usually pale face was flushed and animated.\n",
            "text length: 174201 characters\n",
            "unique characters: 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rqqkzWvttUe"
      },
      "source": [
        "Expected output:  \n",
        "```The Time Traveller (for so it will be convenient to speak of him) was expounding a recondite matter to us. His pale grey eyes shone and twinkled, and his usually pale face was flushed and animated.\n",
        "text length: 174201 characters\n",
        "unique characters: 39```\n",
        "\n",
        "Step 2\n",
        "-----\n",
        "\n",
        "Next we'll divide the text into sequences of 40 characters.\n",
        "\n",
        "Then for each sequence we'll make a training set - the following character will be the correct output for the test set.\n",
        "\n",
        "### In the cell below replace:\n",
        "#### 1. `<sequenceLength>` with `40`\n",
        "#### 2. `<step>` with `4`\n",
        "#### and then __run the code__. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "VWbU1D1mttUg",
        "outputId": "69010f7e-4612-47ee-f89e-6ea5e4396c06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "###\n",
        "# REPLACE <sequenceLength> WITH 40 AND <step> WITH 4\n",
        "###\n",
        "sequence_length = 40\n",
        "step = 4\n",
        "###\n",
        "\n",
        "sequences = []\n",
        "target_chars = []\n",
        "for i in range(0, len(text) - sequence_length, step):\n",
        "    sequences.append([text[i: i + sequence_length]])\n",
        "    target_chars.append(text[i + sequence_length])\n",
        "print('number of training sequences:', len(sequences))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training sequences: 43541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcf_YDrFttU2"
      },
      "source": [
        "Expected output:\n",
        "`number of training sequences: 43541`\n",
        "\n",
        "#### Replace `<addSequences>` with `sequences` and run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "uqc4BpTottU4"
      },
      "source": [
        "# One-hot vectorise\n",
        "\n",
        "X = np.zeros((len(sequences), sequence_length, len(charset)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), len(charset)), dtype=np.bool)\n",
        "\n",
        "###\n",
        "# REPLACE THE <addSequences> BELOW WITH sequences\n",
        "###\n",
        "for n, sequence in enumerate(sequences):\n",
        "###\n",
        "    for m, character in enumerate(list(sequence[0])):\n",
        "        X[n, m, index_from_char[character]] = 1\n",
        "    y[n, index_from_char[target_chars[n]]] = 1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHlxNEtOttVS"
      },
      "source": [
        "Step 3\n",
        "------\n",
        "\n",
        "Let's build our model, using a single LSTM layer of 128 units. We'll keep the model simple for now, so that training does not take too long.\n",
        "\n",
        "### In the cell below replace:\n",
        "#### 1. `<addLSTM>` with `LSTM`\n",
        "#### 2. `<addLayerSize>` with `128`\n",
        "#### 3. `<addSoftmaxFunction>` with `'softmax`\n",
        "#### and then __run the code__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "kjVwgabottVU"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "###\n",
        "# REPLACE THE <addLSTM> BELOW WITH LSTM (use uppercase) AND <addLayerSize> WITH 128\n",
        "###\n",
        "model.add(LSTM(128, input_shape = (X.shape[1], X.shape[2])))\n",
        "###\n",
        "\n",
        "###\n",
        "# REPLACE THE <addSoftmaxFunction> with 'softmax' (INCLUDING THE QUOTES)\n",
        "###\n",
        "model.add(Dense(y.shape[1], activation = 'softmax'))\n",
        "###\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZALHBFattVo"
      },
      "source": [
        "The code below generates text at the end of an epoch (one training cycle). This allows us to see how the model is performing as it trains. If you're making a large neural network with a long training time it's useful to check in on the model as see if the text generating is legible as it trains, as overtraining may occur and the output of the model turn to nonsense.\n",
        "\n",
        "The code below will also save a model if it is the best performing model, so we can use it later.\n",
        "\n",
        "#### Run the code below, but don't change it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "VI3q0IYGttVq"
      },
      "source": [
        "# Run this, but do not edit.\n",
        "# It helps generate the text and save the model epochs.\n",
        "\n",
        "# Generate new text\n",
        "def on_epoch_end(epoch, _):\n",
        "    diversity = 0.5\n",
        "    print('\\n### Generating text with diversity %0.2f' %(diversity))\n",
        "\n",
        "    start = random.randint(0, len(text) - sequence_length - 1)\n",
        "    seed = text[start: start + sequence_length]\n",
        "    print('### Generating with seed: \"%s\"' %seed[:40])\n",
        "\n",
        "    output = seed[:40].lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    print(output, end = '')\n",
        "\n",
        "    for i in range(500):\n",
        "        x_pred = np.zeros((1, sequence_length, len(charset)))\n",
        "        for t, char in enumerate(output):\n",
        "            x_pred[0, t, index_from_char[char]] = 1.\n",
        "\n",
        "        predictions = model.predict(x_pred, verbose=0)[0]\n",
        "        exp_preds = np.exp(np.log(np.asarray(predictions).astype('float64')) / diversity)\n",
        "        next_index = np.argmax(np.random.multinomial(1, exp_preds / np.sum(exp_preds), 1))\n",
        "        next_char = char_from_index[next_index]\n",
        "\n",
        "        output = output[1:] + next_char\n",
        "\n",
        "        print(next_char, end = '')\n",
        "    print()\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "# Save the model\n",
        "checkpoint = ModelCheckpoint('Models/model-epoch-{epoch:02d}.hdf5', \n",
        "                             monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo9xoMc7ttV6"
      },
      "source": [
        "The code below will start the model to train. This may take a long time. Feel free to stop the training with the `square stop button` to the right of the `Run button` in the toolbar.\n",
        "\n",
        "Later in the exercise, we will load a pretrained model.\n",
        "\n",
        "### In the cell below replace:\n",
        "#### 1. `<addPrintCallback>` with `print_callback`\n",
        "#### 2. `<addCheckpoint>` with `checkpoint`\n",
        "#### and then __run the code__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "dggMn4QPttV9",
        "outputId": "d998dbca-f0f4-4df9-d956-977cc71a6a6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "###\n",
        "# REPLACE <addPrintCallback> WITH print_callback AND <addCheckpoint> WITH checkpoint\n",
        "###\n",
        "model.fit(X, y, batch_size = 128, epochs = 3, callbacks = [print_callback, checkpoint])\n",
        "###"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "340/341 [============================>.] - ETA: 0s - loss: 2.7439\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \" settled down into perfect harmony with \"\n",
            " settled down into perfect harmony with tis ono aid ao the he the the tian the aag ae pee a athe d arote aal thas ouwon ahere and the the the pir te aro rad too thin  aie  ir ta ler ao os as thaaid ane oof at s te ai  ouat an iue oame tha ao thenan tos ahe the por we ahe tena ah a on ao ile onl the th o ao io  oualin ad at ale athe boa allit ther the anut ah  hana wam an and the dacie por asr ee the ain ose  ho oo met at ad he the the ins ane san  ouf the s theid orethe he ane aat maes ne an haco aa in thel he as an the o ait awoate  \n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.74363, saving model to Models/model-epoch-01.hdf5\n",
            "341/341 [==============================] - 58s 170ms/step - loss: 2.7436\n",
            "Epoch 2/3\n",
            "340/341 [============================>.] - ETA: 0s - loss: 2.3587\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \"mity now in this old familiar room it is\"\n",
            "mity now in this old familiar room it is ithe ther an the the toe thet ing the s ing and at and the at ind the t at and the the memullit the bad the ane ad the the the the the sout ind the ras ingh iuche the wamed and ine the ind and ind an the the lis ing was woat and ing the the mesit int an mocithe the ind in tere the the sos he ond ind te the thin the s angere ti sid and the anle mame lithind ind wathe the ine onus sathe the be dist the thing aint ile the the the an thonn in the in the the the thind the the the thin the the the so\n",
            "\n",
            "Epoch 00002: loss improved from 2.74363 to 2.35868, saving model to Models/model-epoch-02.hdf5\n",
            "341/341 [==============================] - 57s 167ms/step - loss: 2.3587\n",
            "Epoch 3/3\n",
            "340/341 [============================>.] - ETA: 0s - loss: 2.2315\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \"i opened the door and from within came t\"\n",
            "i opened the door and from within came the of the ming then of fome the stor me han the  “foug sis in the ln the os and ind o the the ther migh ind the pane sof than s the ther and ins and sout s sound on the ind in the the the sas the the the in in i sore l and the cas an and the ing sunt an the gime on af y of the the roond and morer the che the the and and the the an an the the the thin the s on thand the the the poing the cout in the dale i sofet ind the wele fount in in the d and ind and ind the the thing che se the man the the t\n",
            "\n",
            "Epoch 00003: loss improved from 2.35868 to 2.23186, saving model to Models/model-epoch-03.hdf5\n",
            "341/341 [==============================] - 57s 167ms/step - loss: 2.2319\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f70fc474ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9qn4lkctteA"
      },
      "source": [
        "The output won't appear to be very good. But then, this dataset is small, and we have trained it only for a short time using a rather small RNN. How might it look if we upscaled things?\n",
        "\n",
        "Step 5\n",
        "------\n",
        "\n",
        "We could improve our model by:\n",
        "* Having a larger training set.\n",
        "* Increasing the number of LSTM units.\n",
        "* Training it for longer\n",
        "* Experimenting with difference activation functions, optimization functions etc\n",
        "\n",
        "Training this would still take far too long on most computers to see good results - so we've trained a model already for you.\n",
        "\n",
        "This model uses a different dataset - a few of the King Arthur tales pasted together. The model used:\n",
        "* sequences of 50 characters\n",
        "* Two LSTM layers (512 units each)\n",
        "* A dropout of 0.5 after each LSTM layer\n",
        "* Only 30 epochs (we'd recomend 100-200)\n",
        "\n",
        "Let's try importing this model that has already been trained.\n",
        "\n",
        "#### Replace `<addLoadModel>` with `load_model` and run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "njnF0FEatteE",
        "outputId": "cc7db670-d5e1-4c1b-ae92-f435b4003a02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "print(\"loading model... \", end = '')\n",
        "\n",
        "###\n",
        "# REPLACE <addLoadModel> BELOW WITH load_model\n",
        "###\n",
        "model = load_model('/content/arthur-model-epoch-30.hdf5')\n",
        "###\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')\n",
        "###\n",
        "\n",
        "print(\"model loaded\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading model... model loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReQR-TGgtteo"
      },
      "source": [
        "Step 6\n",
        "-------\n",
        "\n",
        "Now let's use this model to generate some new text!\n",
        "\n",
        "#### Replace `<addFilePath>` with `'Data/Arthur tales.txt'`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "4vrvnMn-tteq",
        "outputId": "00383533-515a-4d18-9ddb-eff64b5643a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "###\n",
        "# REPLACE <addFilePath> BELOW WITH 'Data/Arthur tales.txt' (INCLUDING THE QUOTATION MARKS)\n",
        "###\n",
        "text = io.open('/content/drive/My Drive/data/Data/Arthur tales.txt', encoding='UTF-8').read()\n",
        "###\n",
        "\n",
        "# Cut out punctuation and make lower case\n",
        "text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "# Character index dictionary\n",
        "charset = sorted(list(set(text)))\n",
        "index_from_char = dict((c, i) for i, c in enumerate(charset))\n",
        "char_from_index = dict((i, c) for i, c in enumerate(charset))\n",
        "\n",
        "print('text length: %s characters' %len(text))\n",
        "print('unique characters: %s' %len(charset))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text length: 3645951 characters\n",
            "unique characters: 43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTEqzpJMtte3"
      },
      "source": [
        "### In the cell below replace:\n",
        "#### 1. `<sequenceLength>` with `50`\n",
        "#### 2. `<writeSentence>` with a sentence of your own, at least 50 characters long.\n",
        "#### 3. `<numCharsToGenerate>` with the number of characters you want to generate (choose a large number, like 1500)\n",
        "#### and then __run the code__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "0HGqdfWhtte6",
        "outputId": "c969e91a-4293-4a2d-e1bc-3f99a7eb8608",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Generate text\n",
        "\n",
        "diversity = 0.5\n",
        "print('\\n### Generating text with diversity %0.2f' %(diversity))\n",
        "\n",
        "###\n",
        "# REPLACE <sequenceLength> BELOW WITH 50\n",
        "###\n",
        "sequence_length = 50\n",
        "###\n",
        "\n",
        "# Next we'll make a starting point for our text generator\n",
        "\n",
        "###\n",
        "# REPLACE <writeSentence> WITH A SENTENCE OF AT LEAST 50 CHARACTERS\n",
        "###\n",
        "seed = \"i know he is  going to miss me because the love that i gave her is bigger than other else\"\n",
        "###\n",
        "\n",
        "seed = seed.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "###\n",
        "# OR, ALTERNATIVELY, UNCOMMENT THE FOLLOWING TWO LINES AND GRAB A RANDOM STRING FROM THE TEXT FILE\n",
        "###\n",
        "\n",
        "#start = random.randint(0, len(text) - sequence_length - 1)\n",
        "#seed = text[start: start + sequence_length]\n",
        "\n",
        "###\n",
        "\n",
        "print('### Generating with seed: \"%s\"' %seed[:40])\n",
        "\n",
        "output = seed[:sequence_length].lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "print(output, end = '')\n",
        "\n",
        "###\n",
        "# REPLACE THE <numCharsToGenerate> BELOW WITH THE NUMBER OF CHARACTERS WE WISH TO GENERATE, e.g. 1500\n",
        "###\n",
        "for i in range(100):\n",
        "###\n",
        "    x_pred = np.zeros((1, sequence_length, len(charset)))\n",
        "    for t, char in enumerate(output):\n",
        "        x_pred[0, t, index_from_char[char]] = 1.\n",
        "\n",
        "    predictions = model.predict(x_pred, verbose=0)[0]\n",
        "    exp_preds = np.exp(np.log(np.asarray(predictions).astype('float64')) / diversity)\n",
        "    next_index = np.argmax(np.random.multinomial(1, exp_preds / np.sum(exp_preds), 1))\n",
        "    next_char = char_from_index[next_index]\n",
        "\n",
        "    output = output[1:] + next_char\n",
        "\n",
        "    print(next_char, end = '')\n",
        "print()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \"i know he is  going to miss me because t\"\n",
            "i know he is  going to miss me because the love that i should die and then sir percival saw sir launcelot he took his lodging and then they rode toget\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc5qIwfVttfH"
      },
      "source": [
        "How does it look? Does it seem intelligible?\n",
        "\n",
        "Conclusion\n",
        "--------\n",
        "\n",
        "We have trained an RNN that learns to predict characters based on a text sequence. We have trained a lightweight model from scratch, as well as imported a pre-trained model and generated new text from that."
      ]
    }
  ]
}